name: Yahoo RAW batch

on:
  workflow_dispatch:
    inputs:
      tickers:
        description: "Comma-separated tickers (e.g. AAPL,MSFT,NVDA)"
        required: true
        default: "AAPL,MSFT"
      start:
        description: "Start date (YYYY-MM-DD)"
        required: false
        default: "1996-01-01"
      end:
        description: "End date (optional; blank means up to today)"
        required: false
        default: ""

jobs:
  ingest:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install -U pip
          pip install -r requirements.txt || true
          pip install yfinance pandas pyarrow requests supabase

      - name: Ingest via Yahoo RAW batch
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          python - <<'PY'
          import json
          import yfinance as yf
          from data_lake.storage import Storage
          from data_lake.ingest import ingest_raw_yahoo_batch

          # Read input values
          raw = "${{ github.event.inputs.tickers }}"
          start = "${{ github.event.inputs.start }}".strip() or "1996-01-01"
          end = "${{ github.event.inputs.end }}".strip() or None

          # Prepare tickers list
          tickers = [t.strip().upper() for t in raw.split(",") if t.strip()]
          jobs = [{"ticker": t, "start": start, "end": end} for t in tickers]

          print("Jobs:", jobs)

          # Initialize storage object
          s = Storage.from_env()

          # Summary to track results
          summary = {}

          # Loop through each job and attempt to fetch data
          for job in jobs:
              ticker = job["ticker"]
              try:
                  print(f"Fetching data for {ticker}...")

                  # Start by trying the original start date
                  data = yf.download(ticker, start=job["start"], end=job["end"])

                  # If no data is found, try to find the first available data date
                  if data.empty:
                      print(f"No data available for {ticker} starting from {job['start']}. Searching for first available data.")
                      
                      # Fetch the data starting from the earliest possible date and find the first available date
                      temp_data = yf.download(ticker, start="1900-01-01", end=job["end"])
                      if not temp_data.empty:
                          first_valid_date = temp_data.index[0]
                          print(f"First available data for {ticker} starts at {first_valid_date}")
                          data = yf.download(ticker, start=first_valid_date, end=job["end"])

                  if data.empty:
                      print(f"Still no data available for {ticker}. Skipping.")
                      summary[ticker] = "No data"
                  else:
                      # Ingest data if available
                      ingest_raw_yahoo_batch(s, [{"ticker": ticker, "start": str(data.index[0]), "end": str(data.index[-1])}])
                      summary[ticker] = "Success"
              except Exception as e:
                  # Log the error if it fails
                  print(f"Failed to download {ticker}: {str(e)}")
                  summary[ticker] = f"Failed: {str(e)}"

          # Print summary of results
          print(json.dumps(summary, indent=2))

          # Fail only if everything failed
          if all(value == "Failed" for value in summary.values()):
              raise SystemExit("All jobs failed")
          PY
